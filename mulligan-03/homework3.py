# -*- coding: utf-8 -*-
"""Homework3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pm9LJMub_88Ysa4LJxQ6XdBozbnUCGRa

# Question 1 part a
"""

import numpy as np
from matplotlib import pyplot as plt

def genDataSet(N):
    x = np.random.normal(0, 1, N)
    ytrue = (np.cos(x) + 2) / (np.cos(x * 1.4) + 2)
    noise = np.random.normal(0, 0.2, N)
    y = ytrue + noise
    return x, y, ytrue

x, y, ytrue = genDataSet(1000)
plt.plot(x,y,'.')
plt.plot(x,ytrue,'rx')
plt.show()

"""# Question 1 part b and c"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold

kf = KFold(n_splits=10)
X = np.array([x,y]).T
kf.get_n_splits(X)

msek = {}
bestk = 0
bestmse = 100000
for k in np.arange(1,2*np.floor(((len(ytrue)*0.9)+1)/2),2):
  mse = []
  for train_index, test_index in kf.split(X):
    X_train, X_test =     X[train_index],     X[test_index]
    y_train, y_test = ytrue[train_index], ytrue[test_index]

    neigh = KNeighborsRegressor(n_neighbors=int(k))
    neigh.fit(X_train, y_train)
    predictions = neigh.predict(X_test)

    mse.append(mean_squared_error(y_test, predictions))
  msek[k] = np.mean(mse)
  if bestmse > msek[k]:
    bestmse = msek[k]
    bestk = k
    print(bestk,bestmse)

  plt.plot(k,msek[k],'r.')
  plt.xlabel('k')
  plt.ylabel('msek[k]')

print(msek)
plt.show

"""# Question 2
## KNN - Regressor
"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold

kf = KFold(n_splits=10)

bestkall = []
for i in range(100):
  x, y, ytrue = genDataSet(100)
  X = np.array([x,y]).T
  kf.get_n_splits(X)

  msek = {}

  # plt.figure(figsize=(10,10))
  bestk = 0
  bestmse = 100000
  for k in np.arange(1,2*np.floor(((len(ytrue)*0.9)+1)/2),2):
    #print(k)
    mse = []
    for train_index, test_index in kf.split(X):
      X_train, X_test =     X[train_index],     X[test_index]
      y_train, y_test = ytrue[train_index], ytrue[test_index]

      neigh = KNeighborsRegressor(n_neighbors=int(k))
      neigh.fit(X_train, y_train)
      predictions = neigh.predict(X_test)

      mse.append(mean_squared_error(y_test, predictions))

    #print(np.mean(mse))
    msek[k] = np.mean(mse)
    if bestmse > msek[k]:
      bestmse = msek[k]
      bestk = k
      print(bestk,bestmse)
  
  bestkall.append(bestk)
  # plt.plot(k,msek[k],'r.')

# plt.show
print(bestkall)

"""# Question 2 Plot"""

plt.hist(bestkall,13)

"""# Question 3
- Download file
- Download picture
- Change number of colors in line 23
- In line 25 relpace the image file name with the name of your image file
- Run the program, observe the result, answer questions
"""

# Modified by: Pablo Rivas <Pablo.Rivas@Marist.edu>
# on Nov/3/2016
# Original Code:
# http://scikit-learn.org/stable/_downloads/plot_color_quantization.py
# Original Authors:
#          Robert Layton <robertlayton@gmail.com>
#          Olivier Grisel <olivier.grisel@ensta.org>
#          Mathieu Blondel <mathieu@mblondel.org>
#
# License: BSD 3 clause

print(__doc__)
from scipy import misc
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin
from sklearn.datasets import load_sample_image
from sklearn.utils import shuffle
from time import time

# Specify the number of colors here
n_colors = 4

# Load your photo here
img = misc.imread('yankees.jpg')

# Convert to floats instead of the default 8 bits integer coding. Dividing by
# 255 is important so that plt.imshow behaves works well on float data (need to
# be in the range [0-1])
img = np.array(img, dtype=np.float64) / 255

# Load Image and transform to a 2D numpy array.
w, h, d = original_shape = tuple(img.shape)
assert d == 3
image_array = np.reshape(img, (w * h, d))

print("Fitting model on a small sub-sample of the data")
t0 = time()
image_array_sample = shuffle(image_array, random_state=0)[:1000]
kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)
print("done in %0.3fs." % (time() - t0))

# Get labels for all points
print("Predicting color indices on the full image (k-means)")
t0 = time()
labels = kmeans.predict(image_array)
print("done in %0.3fs." % (time() - t0))

def recreate_image(codebook, labels, w, h):
    """Recreate the (compressed) image from the code book & labels"""
    d = codebook.shape[1]
    image = np.zeros((w, h, d))
    label_idx = 0
    for i in range(w):
        for j in range(h):
            image[i][j] = codebook[labels[label_idx]]
            label_idx += 1
    return image

# Display all results, alongside original image
plt.figure(1)
plt.clf()
ax = plt.axes([0, 0, 1, 1])
plt.axis('off')
plt.title('Original image (96,615 colors)')
plt.imshow(img)

plt.figure(2)
plt.clf()
ax = plt.axes([0, 0, 1, 1])
plt.axis('off')
plt.title('Quantized image ('+str(n_colors)+' colors, K-Means)')
plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))

plt.show()

"""# Question 4 - MLP
## 4a
- Download hw3.MLP.sol.py and hw3.MLP.py
- Download hw3_4_a_gendata.py
- Run program for 1000 samples, change values around
"""

import hw3_4_a_gendata
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import KFold


# generates data & split it into X (training input) and y (target output)
X, y = hw3_4_a_gendata.genDataSet(1000)

neurons = 100  # <- number of neurons in the hidden layer
eta = 0.1       # <- the learning rate parameter

# here we create the MLP regressor
mlp =  MLPRegressor(hidden_layer_sizes=(neurons,), verbose=True, learning_rate_init=eta)
# here we train the MLP
mlp.fit(X, y)
# E_out in training
print("Training set score: %f" % mlp.score(X, y))

# now we generate new data as testing set and get E_out for testing set
X, y = hw3_4_a_gendata.genDataSet(10000)
print("Testing set score: %f" % mlp.score(X, y))
ypred = mlp.predict(X)

plt.plot(X[:, 0], X[:, 1], '-')
plt.plot(X[:, 0], y, '-r')
plt.plot(X[:, 0], ypred, '-k')
plt.show()

"""### Class Practice Runs
Changing values of eta
- eta 0.5, bad
- eta 0.05, not bad
- eta 0.01, interesting
- eta 0.001, overfitting more and more every time
- eta 0.0001, BAD
- eta 0.8, BAD

## Question 4 c)
- MLP.sol file
- This varies the value of eta and neurons
"""

import hw3_4_a_gendata
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import KFold

# number of samples
N = 10000

# generate data & split it into X (training input) and y (target output)
X, y = hw3_4_a_gendata.genDataSet(N)

# linear regression solution
w=np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)


#neurons  <- number of neurons in the hidden layer
#eta  <- the learning rate parameter

bestNeurons=0
bestEta=0
bestScore=float('-inf')
score=0
for neurons in range(1,101,1):
  for eta in range(1,11,1):
    eta=eta/10.0
    kf = KFold(n_splits=10)
    cvscore=[]
    for train, validation in kf.split(X):
      X_train, X_validation, y_train, y_validation = X[train, :], X[validation, :], y[train], y[validation]
      # here we create the MLP regressor
      mlp =  MLPRegressor(hidden_layer_sizes=(neurons,), verbose=False, learning_rate_init=eta)
      # here we train the MLP
      mlp.fit(X_train, y_train)
      # now we get E_out for validation set
      score=mlp.score(X_validation, y_validation)
      cvscore.append(score)

    # average CV score
    score=sum(cvscore)/len(cvscore)
    if (score > bestScore):
      bestScore=score
      bestNeurons=neurons
      bestEta=eta
      print("Neurons " + str(neurons) + ", eta " + str(eta) + ". Testing set CV score: %f" % score)

# here we get a new training dataset
X, y = hw3_4_a_gendata.genDataSet(N)
# here we create the final MLP regressor
mlp =  MLPRegressor(hidden_layer_sizes=(bestNeurons,), verbose=True, learning_rate_init=bestEta)
# here we train the final MLP
mlp.fit(X, y)
# E_out in training
print("Training set score: %f" % mlp.score(X, y)) 
# here we get a new testing dataset
X, y = hw3_4_a_gendata.genDataSet(N)
# here test the final MLP regressor and get E_out for testing set
ypred=mlp.predict(X)
score=mlp.score(X, y)
print("Testing set score: %f" % score)
plt.plot(X[:, 0], X[:, 1], '.')
plt.plot(X[:, 0], y, 'rx')
plt.plot(X[:, 0], ypred, '-k')
ypredLR=X.dot(w)
plt.plot(X[:, 0], ypredLR, '--g')
plt.show()

